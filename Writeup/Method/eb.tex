%\documentclass[letterpaper, 12pt]{dfraft}
\documentclass[letterpaper, 12pt]{article}

\setlength\textwidth{5.5in}
%% This is the recommended preamble for your document.

%% Load BEPress specific settings
\usepackage{bejournal}
%\usepackage{caption}
%\usepackage{subcaption}

%% The mathptmx package is recommended for Times compatible math symbols.
%% Use mtpro2 or mathtime instead of mathptmx if you have the commercially
%% available MathTime fonts.
%% Other options are txfonts (free) or belleek (free) or TM-Math (commercial)
\usepackage{mathptmx}

%% Use the graphics package to include figures
\usepackage{graphics}

%% Use natbib with these recommended options
% \usepackage[authoryear,comma,longnamesfirst,sectionbib]{natbib}
\usepackage[authoryear,comma,sectionbib]{natbib}
%\usepackage{natbib}

%\widowpenalty=10000
%\clubpenalty=10000

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}

%\usepackage[usenames]{color}

\usepackage{natbib}

\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Gammad}{Gamma}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\NBP}{NBP}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\prior}{Prior}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sign}
\renewcommand{\Pr}{\mathop{\mathrm{Pr}}\nolimits}

\newcommand{\rev}{{}}
\newcommand{\scA}{{\mathcal{A}}}
\newcommand{\scG}{{\mathcal{G}}}
\newcommand{\scR}{{\mathcal{R}}}

\newcommand{\dpp}[2]{ \dfrac{\partial #1}{\partial #2}}
\newcommand{\ddpp}[2]{ \dfrac{\partial^2 #1}{ {\partial #2}^2}}
\newcommand{\ddppp}[3]{ \dfrac{\partial^2 #1}{ {\partial #2} {\partial #3}} }

%\newcommand{\bP}{\bf P}
\title{Empirical Bayes Estimates of the Dispersion}

\author{Yanming Di}

\begin{document}
%\maketitle

\today
% \currenttime

\begin{center}
{\Large
Empirical Bayes Estimates of the Dispersion
}
\end{center}

Notation: $i$ indexes genes, $j$ indexes samples,
\[ Y_{ij} \sim NB(\mu_{ij}, \phi_{ij}). \]
\[ \mu_{ij} = N_{j} R_{j} \pi_{ij} = N_{j} R_{j} \exp(X' \beta_i). \]
Suppose $\log(\phi_{ij})$ has a prior distribution
\[ \log(\phi_{ij}) = \log(\phi_{ij}^{0}) + \epsilon_{ij}, \]
where $\epsilon_{ij} \sim \N(0, \sigma^2),$
or
\[ \log(\phi_{ij}) = \log(\phi_{ij}^{0}) + \epsilon_i, \]
where $\epsilon_i \sim \N(0, \sigma^2)$ and $\phi_{ij}^{0}$ are estimated from
a preliminary parametric or smooth fit, e.g., the NBQ model.

Note: Gu is using the second model in the simulations. Here we will treat the prior
mean as known, we want to estimate $\sigma^2$.  (For the empirical Bayes
method to work, we need to be able to estimate $\epsilon_i$. The first model
has more parameters than observations.)

\newpage
Denote $\log(\phi_{ij})$ by $\theta_{ij}$, $\log(\phi_{ij}^0)$ by $\theta_0$.
So $\epsilon_i = \theta_{ij} - \theta_{ij}^0$ for all $j$, and $\epsilon_i$
are iid $\N(0, \sigma^2)$.

Let $\theta$ be the collection of all $\theta_{i}$'s.


The unknown parameters in the model (the collections of all NB regressions)
are $\sigma^2$ and $\beta$, the collection of all regression coefficients
$\beta_i$'s.
The joint likelihood of $\sigma^2$ and $\beta$,  which is the joint probability of the
data given $\sigma$ and $\beta_i$, is
\begin{equation}
\begin{tiny}
â€¢
\end{tiny}    \label{eq:likelihood0}
 L(\sigma^2, \beta) = \Pr(y | \sigma^2, \beta) = \prod_i \int \left( \prod_j \Pr(y_{ij} |
\theta_{ij}=\theta_{ij}^0+\epsilon_i, \beta_i) \right)
\prior(\epsilon_i|\sigma^2) d\epsilon_i
\end{equation}
($\beta_i$, $y_i$ are vectors). Note that we have assumed that $\epsilon_i$'s
are independent between genes given $\sigma^2$.

Our goal is to find the MLE of $\sigma^2$. In a frequentist's framework, this
is typically done by finding the joint MLE of $(\sigma^2, \beta)$, which is
equivalent to maximizing the profile likelihood of $\sigma^2$:
\begin{equation}
    \label{eq:likelihood1}
L_p(\sigma^2) = \max_{\beta} L_p(\sigma^2, \beta).
\end{equation}
This seems to be computationally challenging, since we need to find
$\beta$ to maximize an integrated likelihood.

What I will do is to consider profile likelihood of $\epsilon_{i}$ and
replace
\[ \Pr(y_{ij} | \theta_{ij}=\theta_{ij}^0+\epsilon_i, \beta_i) \]
in (\ref{eq:likelihood0}) by
\[ L_p(\epsilon)  = \Pr(y_{ij} | \theta_{ij}=\theta_{ij}^0+\epsilon_i, \hat \beta_i(\theta_i))
\] where $\beta_i(\theta_i)$ means we maximize $\beta_i$ for fixed
$\theta_i$ and consider the integrated likelihood
\[ L_{int}(\sigma^2) = \prod_i \int L_p(\epsilon_i) \prior(\epsilon_i|\sigma^2)
d\epsilon_i  \]
This seems OK (and looks Bayesian), but will not be the same as the profile
likelihood in (\ref{eq:likelihood1}). The subtlety is, from a frequentist
point of view, $\epsilon_i$'s (and thus $\theta_i$'s) are actually latent
data, not parameters.


\newpage
Define the integrated likelihood $\sigma^2$
\[ L_{int}(\sigma^2) = \prod_i \int L_p(\epsilon_i) \Pr(\epsilon_i|\sigma^2) d\epsilon_i  \]
where $L_p(\epsilon_i)$ is the profile log likelihood of $\epsilon_i$ from the
NB regression \[ L_p(\epsilon_i)  = L_p(\epsilon_i; y_i) =  \max_{\beta_i} L(\epsilon_{i}, \beta_i; y_i) =
\max_{\beta_i} \prod_j \Pr(y_{ij}|\theta_{ij}=\theta_{ij}^0 +\epsilon_i,
\beta_i). \]
That is, given $\epsilon_i$ and thus $\theta_{ij}$'s, we need to find the MLE
of $\beta_i$.
(We already have programs for finding the MLE of $\beta_i$ given
$\phi_{ij}$'s. We need to change the parameter to $\epsilon_i$.  We may also
consider the adjusted profile likelihood of $\epsilon_i$.)

Let $l_p(\epsilon_i)$ be the log profile likelihood of $\epsilon_i$.
\[ L_{int}(\sigma^2) = \prod_i \dfrac{1}{ \sqrt{2 \pi \sigma^2}}\int \exp\left(l_p(\epsilon_i; y_i) - \dfrac{\epsilon^2}{2 \sigma^2} \right) d \epsilon_i.  \]
We can approximate each integral using Laplace approximation.


Let $\epsilon_i^*$ maximizes
\[ g(\epsilon_i; y_i) = l_p(\epsilon_i; y_i) - \dfrac{\epsilon_i^2}{2 \sigma^2}, \]
that is
\[ g'(\epsilon_i; y_i) = l_p'(\epsilon_i^*; y_i) - \dfrac{\epsilon_i^*}{\sigma^2} = 0. \]
Then
$\dfrac{1}{ \sqrt{2 \pi \sigma^2}}\int \exp\left(l_p(\epsilon_i; y_i) -
\dfrac{\epsilon^2}{2 \sigma^2} \right) d \epsilon_i$
can be approximated by
\begin{align*}
    &\dfrac{1}{ \sqrt{-\sigma^2 g''(\epsilon_i^*; y_i)}}
    \exp\left(g(\epsilon_i^*; y_i)\right)  \\
    =& \dfrac{1}{ \sqrt{\sigma^2 \left(- l_p''(\epsilon_i^*) + 1/\sigma^2\right)}}
    \exp\left(l_p(\theta_i^*; y_i) - \dfrac{ {\epsilon_i^*}^2}{2 \sigma^2}
    \right).
\end{align*}

For implementation in R, we need to be able to compute $l_p(\epsilon_i; y_i)$,
maximize $g(\epsilon_i; y_i)$ and compute $g''(\epsilon_i^*; y_i)$.
$g''(\epsilon_i^*; y_i)$ can be computed numerically when optimizing $l_p$.
We can also  compute $g''(\epsilon_i^*; y_i)$ analytically:  we already know
how to compute $\ddpp{l_p(\phi_{ij}}{\phi_{ij}}$ (observed information for
$\phi_{ij}$), computing \[l_p''(\epsilon_i^*; y_i) = \sum_j
\ddpp{l_p(\theta_{ij}^*)}{\theta_{ij}},\]  involves a change of variable
step.



Questions: does the normalizing constant in $l_p$ matter?


\newpage
I will focus on one group situation first. (Later, by focusing on $\epsilon_i$
rather than $\phi_{ij}$, we can extend this method to general settings.)
% Denote $\log(\phi_{ij})$ by $\theta_{ij}$,   $\log(\phi_{ij}^{NBQ})$ by
% $\theta_0$. Let $\theta$ be the collection of all $\theta_{ij}$'s.
Denote $\log(\phi_{i})$ by $\theta_{i}$, $\log(\phi_{i}^{NBQ})$ by $\theta_0$.
So $\theta_i \sim \N(\theta_0, \sigma^2)$.
Let $\theta$ be the collection of all $\theta_{i}$'s.

We can derive the profile log likelihood of $\theta$ from the NB regression
model

\[ l_p(\theta) = \sum_i l_p(\theta_i; y_i) = \sum_i \max_{\beta_i} l(\theta_{i}, \beta_i; y_i). \] ($\beta_i$, $y_i$ are vectors).

\[ L(\theta, \sigma^2) = \prod_i \Pr(y_i | \theta_i) \Pr(\theta_i|\sigma^2).  \]
($\beta_i$, $y_i$ are vectors).

In other words, given $\theta_i$, we need to
find the MLE. We already have programs for finding the MLE of $\beta_i$ and
computing the profile likelihood of $\phi_{ij}$. Changing the parameter to
$\theta_i = \log(\phi_i)$ is straightforward.  We can also consider the
adjusted profile likelihood of $\theta$.

The log likelihood $\sigma^2$ or the marginal distribution of the data
given $\sigma^2$ is
\[ L_{int}(\sigma^2) = \prod_i \dfrac{1}{ \sqrt{2 \pi \sigma^2}}\int_\theta
\exp\left(l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2} \right) d\theta.  \]
(We need to assume that, given $\sigma^2$, $\phi$'s are independent.)
We can approximate the summands using Laplace approximation.

Let $\theta_i^*$ maximizes
\[ g(\theta_i; y_i) = l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2}, \]
that is
\[ g'(\theta_i; y_i) = l_p'(\theta_i^*; y_i) - \dfrac{\theta_i^* - \theta_0}{\sigma^2} = 0. \]
The $i$th summand can be approximated by
\begin{align*}
    &\dfrac{1}{ \sqrt{-\sigma^2 g''(\theta_i^*; y_i)}}
    \exp\left(g(\theta_i^*; y_i)\right)  \\
    =& \dfrac{1}{ \sqrt{\sigma^2 \left(- l_p''(\theta_i^*) +
    1/\sigma^2\right)}}
\exp\left(l_p(\theta_i^*; y_i) - \dfrac{(\theta_i^* - \theta_0)^2}{2 \sigma^2} \right)
\end{align*}
Summing this over $i$ give the approximate log likelihood of $\sigma^2$.

For implementation in R, we need to be able to maximize
\[ g(\theta_i; y_i) = l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2} \]
and compute $l_p''(\theta_i^*; y_i)$ (observed information at $\theta_i^*$) and
$l_p(\theta_i^*; y_i)$.

$l_p''(\theta_i^*; y_i)$ can be computed numerically (when optimizing $l_p$)
or analytically.  We already know how to compute $\ddpp{l_p}{\phi_i}$

Questions: does the normalizing constant in $l_p$ matter?

\newpage
I will focus the one group situation first. (Later, by focusing on
$\epsilon_i$, we can extend this method to general settings.)
% Denote $\log(\phi_{ij})$ by $\theta_{ij}$,   $\log(\phi_{ij}^{NBQ})$ by
% $\theta_0$. Let $\theta$ be the collection of all $\theta_{ij}$'s.
Denote $\log(\phi_{i})$ by $\theta_{i}$, $\log(\phi_{i}^{NBQ})$ by $\theta_0$.
So $\theta_i \sim \N(\theta_0, \sigma^2)$.
Let $\theta$ be the collection of all $\theta_{i}$'s.

We can derive the profile log likelihood of $\theta$ from the NB regression
model \[ l_p(\theta) = \sum_i l_p(\theta_i; y_i) = \sum_i \max_{\beta_i} l(\theta_{i}, \beta_i; y_i). \]
($\beta_i$, $y_i$ are vectors.) In other words, given $\theta_i$, we need to
find the MLE. We already have programs for finding the MLE of $\beta_i$ and
computing the profile likelihood of $\phi_{ij}$. Changing the parameter to
$\theta_i = \log(\phi_i)$ is straightforward.  We can also consider the
adjusted profile likelihood of $\theta$.

The log likelihood $\sigma^2$ or the marginal distribution of the data
given $\sigma^2$ is
\[ \sum_i \dfrac{1}{ \sqrt{2 \pi \sigma^2}}\int_\theta
\exp\left(l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2} \right) d\theta.  \]
We can approximate the summands using Laplace approximation.

Let $\theta_i^*$ maximizes
\[ g(\theta_i; y_i) = l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2}, \]
that is
\[ g'(\theta_i; y_i) = l_p'(\theta_i^*; y_i) - \dfrac{\theta_i^* - \theta_0}{\sigma^2} = 0. \]
The $i$th summand can be approximated by
\begin{align*}
    &\dfrac{1}{ \sqrt{-\sigma^2 g''(\theta_i^*; y_i)}}
    \exp\left(g(\theta_i^*; y_i)\right)  \\
    =& \dfrac{1}{ \sqrt{\sigma^2 \left(- l_p''(\theta_i^*) +
    1/\sigma^2\right)}}
\exp\left(l_p(\theta_i^*; y_i) - \dfrac{(\theta_i^* - \theta_0)^2}{2 \sigma^2} \right)
\end{align*}
Summing this over $i$ give the approximate log likelihood of $\sigma^2$.

For implementation in R, we need to be able to maximize
\[ g(\theta_i; y_i) = l_p(\theta_i; y_i) - \dfrac{(\theta_i - \theta_0)^2}{2 \sigma^2} \]
and compute $l_p''(\theta_i^*; y_i)$ (observed information at $\theta_i^*$) and
$l_p(\theta_i^*; y_i)$.

$l_p''(\theta_i^*; y_i)$ can be computed numerically (when optimizing $l_p$)
or analytically.  We already know how to compute $\ddpp{l_p}{\phi_i}$

Questions: does the normalizing constant in $l_p$ matter?


\newpage
\section*{General theory}
Let $l_i(\theta_i)$ be the likelihood of $\theta_i$ from of the $i$th gene and
assume $l_i(\theta_i) = \exp(-f(\theta_i))$. In an empirical Bayes approach,
we assume $\theta_i$ has some prior distribution, say $\theta_i \sim
\N(\theta_0, \sigma^2)$. The marginal distribution of the data (also the
likelihood of $\sigma^2$) is then \[ \dfrac{1}{ \sqrt{2 \pi
\sigma^2}}\int_\theta \exp\left(-f(\theta_i) - \dfrac{(\theta_i -
\theta_0)^2}{2 \sigma^2} \right) d\theta.  \]


The key idea is that we can approximate the integral using Laplace approximation:
If $g(\theta)$ has a minimum at $\theta^*$, then
\[ \int_\theta \exp(-g(\theta)) d\theta \approx  \exp(-g(\theta^*)) \sqrt{2 \pi /
g''(\theta^*)}. \]
This approximation is exact if $g(\theta)$ is a quadratic function of $theta$.


In this problem,
\[ g(\theta) = f(\theta) + \dfrac{(\theta - \theta_0)^2}{2 \sigma^2}. \]
Let $\theta_i^*$ be the solution to
\[ g'(\theta) = f'(\theta_i) + \dfrac{\theta_i - \theta_0}{\sigma^2} =0. \]
Then the integral is approximately
 \[ \dfrac{1}{ \sqrt{\sigma^2 g''(\theta_i^*)}} \exp\left(-g(\theta_i^*)\right). \]
where
 \[ g''(\theta^*) = f''(\theta^*) + \dfrac{1}{\sigma^2} = j(\theta^*) +
 \dfrac{1}{\sigma^2}. \]

 The likelihood of $\sigma^2$ from all genes combined is \[ \sum_i
 \dfrac{1}{ \sqrt{\sigma^2 g''(\theta_i^*)}}
 \exp\left(-g(\theta_i^*)\right). \]
 Note that $\theta_i^*$ depends on $\sigma^2$.

 To compute the likelihood of $\sigma^2$,
 \begin{enumerate}
     \item
     \item
 \end{enumerate}


When $f(\theta_i)$ corresponds to a normal distribution (of $\hat\theta$ for
example) with variance $\sigma_i^2$.  Then  $f(\theta) = (\hat\theta -
\theta)^2/2 \sigma_i^2$, $\theta^* = \dfrac{\dfrac{\hat\theta_i}{\sigma_i^2} +
\dfrac{\theta_0}{\sigma^2}}{\dfrac{1}{\sigma_i^2} + \dfrac{1}{\sigma^2}}$,
and the marginal distribution of $\theta_0$ is \[ \dfrac{1}{  \sqrt{2 \pi
(\sigma_i^2+\sigma^2)}} \exp\left(-
\dfrac{(\hat\theta-\theta_0)^2}{2(\sigma_i^2+\sigma^2)} \right). \]


%  f(\hat \theta_i) - \dfrac{(\hat \theta_i - \theta_0)^2}{2 \sigma^2}
%  \right) \] The key \[ \dfrac{1}{ \sqrt{2 \pi \sigma^2}} \exp\left(-f(\hat
%  \theta_i) - \dfrac{(\hat \theta_i - \theta_0)^2}{2 \sigma^2} \right) \]
%  The key

\end{document}


% Authors: Yanming Di, Sarah C. Emerson, Daniel W. Schafer, Jeffrey A. Kimbrel, Jeff H. Chang

\section*{List of Notation}

\begin{tabular}{ll}
    $i = 1, \dots, m$ & indexes tags (genes, exons, etc.). \\
    $j = 1, \dots, n$ & indexes libraries. \\
    $k = 1, \dots, p$ & indexes covariates or treatment groups. \\
    $d = 0, \dots, D$ & indexes regression coefficients $\alpha$ in the dispersion model. \\
    $Y_{ij}, y_{ij}$ & read counts\\
    $X_{jk}, x_{jk}$ & measured covariates \\
    % $F_{ij} = Y_{ij}/N_j$ & relative counts \\
    % $S_i$ & row sums. \\
    $R_j$ & normalization factors. \\
    $N_j^* = N_j R_j$ & effective library sizes. \\
    $\pi_{ij} = \E(Y_{ij}) / N_j^*$ & expected relative counts, mean parameters of the NB model\\
    $\mu_{ij} = N_j^* \pi_{ij} = \E(Y_{ij})$ & expected counts, mean parameters of the NB model\\
    $\phi_{ij}$ & NB dispersion parameters \\
    $\kappa_{ij} = 1/\phi_{ij}$ & size/shape parameter of the NB model \\
\end{tabular}

\newpage
\section{NB Regression Model}
\label{sec:model}
% \subsection{Summary of the Model}
Let $Y_{ij}$ represent the number of RNA-Seq reads from biological sample $j$
attributed to gene $i$ and let $X_{jk}$ be the value of the $k$-th explanatory
variable associated with biological sample $j$, for $i=1,\dots, m$;
$j=1,\dots,n$; and $k=1,\dots,p$.  Let $N_j$ be the total number of
unambiguously aligned sequencing reads associated with biological sample
$j$ (i.e., $N_j = \sum_{i=1}^m Y_{ij}$), which we refer to as the {\em
(observed) library size} of sample $j$.  Our NB regression model for
describing the mean expression of gene $i$ as a function of explanatory
variables includes the following three components:
\begin{enumerate}
    \item
	A NB probability distribution for the frequency of reads:
	\begin{equation}
	    \label{eq:nb}
	    Y_{ij} \sim \NB(\mu_{ij}, \phi_{ij}),
	\end{equation}
	where $\mu_{ij}$ is the mean and $\phi_{ij}$ is the NB {\em dispersion
	parameter} such that $\var(Y_{ij}) = \mu_{ij} + \phi_{ij}
	\mu_{ij}^2$.
	We assume that frequencies indexed by different $i$'s and $j$'s are
	independent of one another.
    \item
	A log-linear regression model for the mean as a function of
	explanatory variables
	% \[ Y_{ij} \sim \NB(\mu_{ij}, \kappa_{ij} = 1/\phi_{ij}), j=1, \dots, J, \]
	\begin{equation}
	    \label{eq:mu}
	    \log(\mu_{ij})
	    =\log(N_j) + \log(R_j) + X_j^T \beta_i
	    =\log(N_j) + \log(R_j) + \sum_{k=1}^p X_{jk} \beta_{ik}
	\end{equation}
	where $\beta_{ik}$ are unknown regression coefficients associated with
	gene $i$ and the $R_j$'s are optional normalization factors, explained
	below.
    \item
	A model for the dispersion parameter $\phi_{ij}$ as a function of the
	relative mean $\pi_{ij}= \mu_{ij}/(N_jR_j)$:
	\begin{equation}\label{eq:shape}
	    \log(\phi_{ij}) = f(\pi_{ij}, \alpha);
	\end{equation}
	where $f(\cdot)$ is a specified parametric model, such as a
	polynomial,
	\begin{equation}
	    \label{eq:dispersion}
	    \log(\phi_{ij})  = \sum_{d=0}^D \alpha_d (\log(\pi_{ij}))^d,
	  %   f(\pi_{ij}, \alpha) = \alpha_0 + \alpha_1 \log(\pi_{ij}) + \alpha_2(\log(\pi_{ij}))^2,
	\end{equation}
	  ($D=0, 1, 2$ corresponds to the NB2, NBP and NBQ models correspondingly),
	or the result of a nonparametric regression fitting algorithm.
\end{enumerate}

\section{Log Likelihood of a Single NB Random Variable and Its Derivatives}
The probability mass function of a single NB random variable $Y$ with mean
$\mu$ and size/shape parameter $\kappa$ (the reciprocal of the dispersion $\phi$) is
\[
\Pr(Y=y; \mu, \kappa) = \dfrac{\Gamma(\kappa+y)}{\Gamma(\kappa)\Gamma(1+y)} \left(\dfrac{\mu}{\mu+\kappa}\right)^y
\left(\dfrac{\kappa}{\mu+\kappa}\right)^\kappa. \]


The log likelihood of $(\mu, \kappa)$ from a single observation is
\begin{equation}
   \label{eq:log-likelihood-mu-kappa}
l(\mu, \kappa; y)
=\log(\Gamma(\kappa+y))-\log(\Gamma(\kappa)) + y \log(\mu)  +\kappa \log(\kappa) - (y+\kappa)\log(\mu+\kappa).
\end{equation}

The first derivatives of the log likelihood with respect to $\mu$ and $\kappa$ are
\[ \dpp{l}{\mu} = \dfrac{y}{\mu} - \dfrac{\kappa +y}{\mu+\kappa}
  = \dfrac{(y - \mu)\kappa}{\mu(\mu+\kappa)}
 = \dfrac{y - \mu}{\sigma^2}, \]
where $\sigma^2 = \mu + \dfrac{1}{\kappa}\mu^2$,
and
\begin{align*}
 \dpp{l}{\kappa} &=
\Psi \left( \kappa+y \right) -\Psi \left( \kappa \right) +\ln  \left(
\kappa \right) +1-\ln  \left( \mu+\kappa \right) -{\frac
{\kappa+y}{\mu+\kappa}} \\
&= \Psi \left( \kappa+y \right) -\Psi \left( \kappa \right) +\ln  \left(
\kappa \right) - \ln  \left( \mu+\kappa \right) -{\frac
{y - \mu}{\mu+\kappa}},
\end{align*}
where $\Psi$ is the digamma function.

The second derivatives are
\[ \ddpp{l}{\mu} = -\frac {y}{\mu^2}+\frac {\kappa+y}{ \left( \mu+\kappa
\right) ^ 2}, \]
\[ \ddpp{l}{\kappa} =
\Psi_1 \left( \kappa+y \right) -\Psi_1 \left( \kappa \right) + \dfrac{1}{\kappa}
- \dfrac{2}{\mu+\kappa} + \dfrac{\kappa+y}{(\mu+\kappa)^2}, \]
\[ \ddppp{l}{\kappa}{\mu} =
- \dfrac{1}{\left( \mu+\kappa \right)}+{\frac {\kappa+y}{ \left( \mu+\kappa
\right) ^{2}  } = \dfrac{y - \mu}{(\mu+\kappa)^2}.
 \]



\section{Sum of i.i.d.\ NB random variables}
If $Y_j, j= 1, \dots, n$, are iid NB random variables with mean $\mu$ and size $\kappa$,
then $\sum Y_j$ is a NB random variable with mean $n \mu$ and size $n \kappa$.
(The moment generating function of a NB random variable is
\[ \left( \dfrac{1-p}{1-pe^t} \right)^\kappa, \]
where $p = \dfrac{\kappa}{\kappa+\mu}$.)

When $\kappa$ is known, the joint distribution of $(Y_1, \dots, Y_n)$ belongs
to an one-dimensional full-rank exponential family. $\sum_{i=1}^n Y_i$ is the
complete sufficient statistic for $\mu$. $\bar Y = \frac{1}{n}\sum_{i=1} Y_i$
is the MLE and UMVUE of $\mu$.

The log conditional probably of $(Y_1, \dots, Y_n)$ given $\sum_{j=1}^n Y_i = s$ is
\[ \sum_j \left[ \log(\Gamma(\kappa+y_j))-\log(\Gamma(\kappa)) - \log(1+y_j) \right]
- \left[ \log(\Gamma(n \kappa + s))-\log(\Gamma(n \kappa)) - \log(1+s) \right]
\]
The log conditional likelihood of $\kappa$ is thus (up to a constant that
depends on $y_j$'s but does not depend on $\kappa$):
\[ l_c = \sum_j \left[ \log(\Gamma(\kappa+y_j))-\log(\Gamma(\kappa)) \right] -
\left[ \log(\Gamma(n \kappa + s))-\log(\Gamma(n \kappa)) \right]
\]
Note that this conditional likelihood does not depend on $\mu$ (since
$\sum_{j=1}^n Y_j$ is sufficient for $\mu$.

The derivative of this log conditional likelihood with respect to $\kappa$ is
\[ \dpp{l_c}{\kappa} = \sum_j \left[ \Psi(\kappa+y_j)- \Psi(\kappa) \right] -
\left[ n \Psi(n \kappa + s)- n \Psi(n \kappa) \right]
\]
If $\theta = \log(\kappa)$ is the parameter, $\dpp{l_c}{\theta} =
\dpp{l_c}{\kappa(\theta)} \kappa(\theta)$.

The second derivative of the log conditional likelihood with respect to $\kappa$ is
\[ \ddpp{l_c}{\kappa} = \sum_j \left[ \Psi_1(\kappa+y_j)- \Psi_1(\kappa) \right] - \left[ n^2 \Psi_1(n \kappa + s)- n^2 \Psi_1(n \kappa) \right] \]


\section{Log Likelihood of the NB Regression Model}
The log likelihood of $(\alpha, \beta)$ in the NB regression model is given by
summing the likelihood of $(\mu_{ij}, \kappa_{ij})$ (see
equation~(\ref{eq:log-likelihood-mu-kappa}))
\[ l(\mu_{ij}, \kappa_{ij}; y_{ij})
=\log(\Gamma(\kappa_{ij}+y_{ij}))-\log(\Gamma(\kappa_{ij})) + y_{ij}
\log(\mu_{ij})  +\kappa_{ij} \log(\kappa_{ij}) -
(y_{ij}+\kappa_{ij})\log(\mu_{ij}+\kappa_{ij}). \]
over all observations
\[ l(\alpha, \beta) = \sum_{i=1}^m \sum_j^n l (\mu_{ij}, \kappa_{ij}; y_{ij}) \]
after expressing $\mu_{ij}$ and $\kappa_{ij}$ in terms of $\alpha$ and $\beta$
according to the NB regression model (\ref{eq:mu}) and the dispersion
model~(\ref{eq:dispersion}).
Note that $\alpha$ has $d+1$ components
$\beta = \{\beta_{ik}\}_{i=1,\dots,m; k=1,\dots,p}$ has $m \times p$
components.

We will use $l$, $l_i$, and $l_{ij}$ as shorthands for the likelihood of
$(\alpha, \beta)$ from all observations, observations in row $i$, and a single
observation $ij$ respectively, so $l = \sum_i l_i = \sum_i \sum_j l_{ij}$.


\section{First Derivatives of Log Likelihood}

The first derivative of $l$ with respect to $\beta_{ik}$ depends only on
observations in row $i$:
\begin{align}
    \dpp{l}{\beta_{ik}} &= \dpp{l_i}{\beta_{ik}} \\
    & = \sum_j \dpp{l_{ij}}{\kappa_{ij}} \dpp{\kappa_{ij}}{\log{\kappa_{ij}}}
    \dpp{\log{\kappa_{ij}}}{\log{\pi_{ij}}} \dpp{\log{\pi_{ij}}}{\beta_{ik}}
     + \sum_j \dpp{l_{ij}}{\mu_{ij}} \dpp{\mu_{ij}}{\log{\mu_{ij}}}
     \dpp{\log{\mu_{ij}}}{\beta_{ik}}  \\
     & = \sum_{j=1}^n \dpp{l_{ij}}{\kappa_{ij}} {\kappa_{ij}}
\dpp{\log{\kappa_{ij}}}{\log{\pi_{ij}}}
x_{jk}
    +
    \sum_{j=1}^n \dfrac{y_{ij} - \mu_{ij}}{\sigma_{ij}^2} \mu_{ij} x_{jk},
\end{align}
where
\[\dpp{\log{\kappa_{ij}}}{\log{\pi_{ij}}} =-\sum_{d=1}^D \alpha_d
(\log(\pi_{ij}))^{d-1} \]
and
\[ \dpp{l_{ij}}{\kappa_{ij}} =
   \Psi \left( \kappa_{ij}+y_{ij} \right) -\Psi \left( \kappa_{ij} \right) +\ln  \left(
    \kappa_{ij} \right) - \ln  \left( \mu_{ij}+\kappa_{ij} \right) -{\frac
    {y_{ij} - \mu_{ij}}{\mu_{ij}+\kappa_{ij}}}. \]
% or in vector form
% \[ \dpp{l_i}{\beta_{i.}} = \sum_j \dpp{l_{ij}}{\mu_{ij}}
% \dpp{\mu_{ij}}{\log{\mu_{ij}}} \dpp{\log{\mu_{ij}}}{\beta_{i.}} = \sum_{j=1}^n \dfrac{y_{ij} - \mu_{ij}}{\sigma_{ij}^2} \mu_{ij} x_{i.}, \]

The first derivative of $l$ with respect to $\alpha_d$ depends on ALL observations.
\begin{align}
    \dpp{l}{\alpha_d}
    & = \sum_i \sum_j \dpp{l_{ij}}{\kappa_{ij}}
    \dpp{\kappa_{ij}}{\log{\kappa_{ij}}} \dpp{\log{\kappa_{ij}}}{\alpha_d} \\
    & = \sum_i \sum_j \dpp{l_{ij}}{\kappa_{ij}} \kappa_{ij} (-
    (\log(\pi_{ij}))^d)
\end{align}

\section{Finding MLE of $(\alpha, \beta)$}

Under an NBP model, $\kappa = \phi_0^{-1} \pi^{2-\alpha} = \phi_0^{-1}
(\mu/N)^{2-\alpha}$.  The first
derivatives of $\eta$ and $\kappa$ with respect to $\mu$ are
\[ \dpp{\kappa}{\mu} = (2-\alpha)\dfrac{\kappa}{\mu}, \]
\[ \dpp{\eta}{\mu} = \dfrac{1}{\mu} - \dfrac{1}{\mu + \kappa} (1 +
\dpp{\kappa}{\mu}) = \dfrac{\alpha - 1}{\sigma^2},\]
where $\sigma^2 = \mu + \dfrac{1}{\kappa} \mu^2$ is the variance of $Y$.
So $\dpp{l}{\mu}$ is the sum of
\[ \dpp{l}{\eta}\dpp{\eta}{\mu}
%=\dfrac{y-\mu}{(\mu/\kappa+1)\mu}
= (\alpha-1)\dfrac{y - \mu}{\sigma^2}\]
and
\[ \dpp{l}{\kappa} \dpp{\kappa}{\mu} = (2 - \alpha) \dfrac{\kappa}{\mu} (\Psi(\kappa+y)- \Psi(\kappa) + \log(1-p)). \]
(When $\alpha=2$, the density function belongs to an exponential family of
distributions and $\eta$ is the natural parameter and $\dpp{l}{\mu} = \dfrac{y
- \mu}{\sigma^2}$.)

% \[ \dpp{l}{p} =-\dfrac{\kappa}{1-p}+\dfrac{y}{p}, \]
% \[ \ddpp{l}{p} =-\dfrac{\kappa}{(1-p)^2}-\dfrac{y}{p^2}, \]
% \[ \dpp{p}{\mu} = \kappa/(\mu+\kappa)^2 =p(1-p)/\mu, \]
% \[ \partial \kappa/\partial
% \mu=\dfrac{(2-\alpha)\mu^{2-a}}{\phi\mu}=(2-a)\kappa/\mu.\]

To compute the observed Fisher information matrix $\ddpp{l}{\mu}$, note that
\[ \ddpp{l}{\mu}=
\dpp{(\eta, \kappa)}{\mu}
\begin{pmatrix}
        \ddpp{l}{\eta} & \ddppp{l}{\eta}{\kappa}  \\
	    \ddppp{l}{\eta}{\kappa} & \ddpp{l}{\kappa}
	\end{pmatrix}
	\dpp{(\eta, \kappa)^T}{\mu}
	+ \dpp{l}{\eta}\ddpp{\eta}{\mu}
	+ \dpp{l}{\kappa}\ddpp{\kappa}{\mu}.
	\]
	The following second derivatives are needed:
	\[ \ddpp{l}{\eta} =  -\dfrac{\kappa e^\eta}{(1 - e^\eta)^2} =
	-\sigma^2, \]
	\[ \ddppp{l}{\eta}{\kappa} =  -\dfrac{e^\eta}{1 - e^\eta} = -
	\dfrac{\mu}{\kappa}, \]
	\[ \ddpp{l}{\kappa} = \Psi_1(\kappa+y)- \Psi_1(\kappa), \]
	$\Psi_1$ is the trigamma function, and
	% \[ \ddpp{l}{\mu}=\ddpp{l}{\eta}(\dpp{\eta}{\mu})^2 +
	% \dpp{l}{\eta}\ddpp{\eta}{\mu}
	% = -\dfrac{1}{\sigma^2} - (y-\mu)\dfrac{1}{\sigma^4}
	% (\dfrac{2\mu}{\kappa} + 1)\]
	\[ \ddpp{\kappa}{\mu} = (2-\alpha)(1-\alpha) \dfrac{\kappa}{\mu^2}. \]
	\[ \ddpp{\eta}{\mu} = -\dfrac{1}{\mu^2} +
	\dfrac{(1 + \dpp{\kappa}{\mu})^2}{(\mu + \kappa)^2}
	-\dfrac{\ddpp{\kappa}{\mu}}{\mu + \kappa}
	, \]
	%= -\dfrac{1} {\sigma^4} (\dfrac{2\mu}{\kappa} + 1),\]
	The derivatives with respect to $\pi$ are simply
	$\dpp{l}{\pi}=N\dpp{l}{\mu}$
	and $\ddpp{l}{\pi} = N^2\ddpp{l}{\mu}$.



If a parametric model $\log(\phi) = f(\mu; \alpha)$ is assumed, then the
likelihood ratio test statistic for a hypothesis about $\beta$ (the vector of
regression coefficients for all genes) is $2l(\hat\alpha, \hat\beta)
-2l(\tilde\alpha, \tilde\beta)$, where $l( )$ is the log-likelihood function
for counts from all genes combined (and treated as independent); $\hat\alpha$
and $\hat\beta$ are the unconstrained maximum likelihood estimates; and
$\tilde\alpha$ and $\tilde\beta$ are the maximum likelihood estimates for the
null-constrained model.  Because the maximization based on all genes is
unwieldy, it is more practical to approximate the MLEs by a one- or
two-iteration two-step process in which the $\beta$'s for each gene are
estimated individually with the estimated $\alpha$ taken to be known and with
$\alpha$ estimated from all genes based on estimated $\beta$'s.  A further
simplifying approximation involves estimating $\beta$'s for each gene
individually by treating  the estimated $\phi$'s as known, via the estimated
model $\log(\hat\phi) = f(\hat\mu; \hat\alpha)$.

Although there is additional uncertainty in this latter approximation due to
the small-sample estimation of $\hat\mu$ for each gene, this is the
approximation that we will investigate in this paper.  It permits the
incorporation of the proposed HOA-adjusted likelihood ratio tests into the
edgeR package \citep{Robinson:McCarthy:Smyth:2010}, which uses empirical Bayes
estimates of $\phi$'s, optionally with an underlying trend of $\phi$ as a
function of $\mu$; the DESeq package \citep{Anders:Huber:2010}, which estimates
the $\phi$'s as a function of estimated $\mu$'s using nonparametric
regression; and the NBPSeq approach \citep{Di:etal:2011}, which conducts
likelihood analysis for the NBP parameterization of the NB model, in which the
log of $\phi$ is a straight line function of the log of the mean. Our
simulations suggest that the HOA adjustment is useful even with this
approximation.  We discuss the logical next steps in the evolution of negative
binomial regression for RNA-Seq data analysis in Section~\ref{sec:conclusion},
with particular attention to the ``known $\phi$'' issue.



\section{The Likelihood Ratio Test and Higher Order Asymptotic Adjustment}
\label{sec:hoa}
The model in Section 2 implies a NB log-linear regression model
for each gene, $i$. We suppress the index $i$ here:
\begin{quote}
    \[ Y_j \sim NB(\mu_j, \phi_j)\]
    \[ \log(\mu_{j}) = \log(N_j) + \log(R_j) + \sum_{k=1}^p \beta_k X_{jk} \]
\end{quote}
with the $\phi_j$'s taken to be known. We wish to test hypotheses about
components of $\beta$, the vector of regression coefficients.
Without loss of generality, suppose that $\beta=(\psi, \nu)$, where
$\psi=(\beta_1, \dots, \beta_q)$ and $\nu=(\beta_{q+1}, \dots, \beta_p)$, and
the null hypothesis is $\psi=\psi_0$. In regard to this hypothesis, the
$q$-dimensional parameter $\psi$ is the parameter of interest and $\nu$ is a
nuisance parameter.  We let $\hat\beta(y) = \hat\beta = (\hat\psi, \hat\nu)$
be the maximum likelihood estimator of the full parameter vector and $\tilde
\beta (y) = \tilde\beta = (\tilde \nu, \psi_0)$ be the maximum likelihood
estimator under the null hypothesis.

% Classical asymptotic theory dates back to Fisher (1922, 1925).
Under the usual regularity conditions, the likelihood ratio statistic,
\[ \lambda = 2 (l(\hat\beta) - l(\tilde\beta)), \]
converges in distribution to a chi-square distribution with degrees of freedom
$q$ under the null hypothesis \citep{Wilks:1938}.
% This asymptotic result is the basis of the familiar likelihood ratio test (LRT).
When $\psi$ is one-dimensional ($q=1$), the signed square root of the
likelihood ratio statistic $\lambda$, also called the directed deviance,
\begin{equation}
    \label{eq:r}
    r = \sign(\hat\psi - \psi_0) \sqrt \lambda,
\end{equation}
converges to a standard normal distribution. The latest versions of DESeq
\citep{Anders:Huber:2010} and edgeR \citep{Robinson:McCarthy:Smyth:2010} both
implemented the unadjusted likelihood ratio test for coefficients in NB
regression models, but the two packages differ in how they pool information
across genes to estimate dispersion parameters. Another R package BBSeq
\citep{Zhou:etal:2011} also used unadjusted likelihood ratio test for
regression coefficients, but BBSeq models the count variation using a
beta-binomial model instead of a negative binomial model.

% Wilk's test is invariant to reparameterizations of the model and the hypothesis.
% We will refer the test based on $r^*$ with Skovgaard's approximation as the
%  higher-order asymptotic (HOA) test.
For testing a one-dimensional parameter of interest ($q=1$),
\citet{Barndorff-Nielsen:1986, Barndorff-Nielsen:1991} showed that a {\em modified directed
deviance}
\begin{equation}
    \label{eq:rstar}
    r^* = r - \dfrac{1}{r} \log(z)
\end{equation}
is, in wide generality, asymptotically standard normally distributed to a
higher order of accuracy than the directed deviance $r$ itself, where $z$ is
an adjustment term to be discussed below.
% A major idea behind this modification is that $\hat\beta$ in general is not
% sufficient and the inference for $\beta$ can be made more accurate by
% conditioning on an ancillary statistic together with which $\hat \beta$ is
% sufficient.  To make it conceptually more illuminating, Pierce and Peters ()
% suggest that one can partition the adjustment term $z$ into two components,
% one correcting for the information loss due to the MLE being not sufficient
% and the other reducing the effect of estimating the nuisance parameters.
Tests based on high-order asymptotic adjustment to the likelihood ratio
statistic, such as  $r^*$ or its approximation (explained below), are referred
to as higher-order asymptotic (HOA) tests. They generally have better accuracy
than corresponding unadjusted likelihood ratio tests, especially in situations
where the sample size is small and/or when the number of nuisance parameters
($p - q$) is large.


\citet{Skovgaard:2001} gave a comprehensive review of the development of the
theory and practice of higher order asymptotics. That paper also presented a
generalization of Barndorff-Nielsen's $r^*$ statistic to test for multi-dimensional parameters ($q>1$). The test statistic
\begin{equation}
    \label{eq:lambda-star}
    \lambda^{**} = \lambda(1-\dfrac{1}{\lambda}\log\gamma)^2
\end{equation}
is constructed by adding an HOA adjustment to the likelihood ratio statistic.  In
Appendix~\ref{app:hoa}, we provide implementation details of the HOA test with
Skovgaard's approximations for one-dimensional and multi-dimensional
parameters in the context of the NB regression model.

% \subsection{Overview}
We present examples and numerical results to clarify the regression part of
the model~(\ref{eq:mu}) and to compare the performance of three asymptotic
tests: the HOA test based on the $r^*$ statistic (\ref{eq:rstar}), the
unadjusted likelihood ratio (LR) test based on the $r$ statistic
(\ref{eq:r}), and one other commonly used large sample test based on
% the score test statistic \[ U = \tilde D_1^T \tilde i^{-1} \tilde D_1 \]
% and
the Wald statistic \citep{Wald:1941, Wald:1943}, \[(\hat\psi - \psi_0)^T([\hat
i(\beta) ^{-1}]_{\psi\psi})^{-1} (\hat\psi - \psi_0),\]
where $i(\beta)$ is the Fisher (expected) information. The $p$-values reported
by the {\tt glm.nb} program in the R package MASS are Wald test $p$-values.
Under the null hypothesis, all three of these tests (HOA, LR, and Wald) have
the same asymptotic chi-square distribution with $q$ degrees of freedom.

% In \citet{Di:etal:2011}, we analyzed an Arabidopsis data set. We challenged
% leaves of


\subsection{Consequences of Treating Dispersion Estimates as Known}
\label{sec:type-I-set}

In the type I error and power simulations in
Section~\ref{sec:type-I-simulations} and \ref{sec:power-simulations}, we used
the actual (data generating) value of the dispersion $\phi$ in place of the
estimate for evaluating the tests.  In practice, we would estimate a model for
$\phi$ from all genes combined, then estimate $\phi$ for each individual gene
from this fit and from the estimated $\mu$ for the particular gene, and would
treat the estimated $\phi$'s as known in tests for regression coefficients.  A
more practically realistic simulation strategy would have been to simulate
counts from a complete set of genes (such as 25,000 genes) in each Monte Carlo
step,  but that strategy would have been very time consuming.
% To understand the consequences of inserting the true value of $\phi$, we
% conducted this larger type of simulation under a two-group setting.  for a
% subset of conditions.  In this, we found the type I error rates were
% essentially identical to those using the simplification.

% In an attempt
To understand the consequences of treating estimated $\phi$'s as known, we
simulated one complete set of genes under a two-group comparison setting and
estimated the dispersion model from all genes combined (details given below).
% In this initial investigation, When the estimated values of $\phi$'s were
% used in the tests,
In this, we found that the type I error rates of the three large sample tests
did not depend very much on whether true or estimated $\phi$'s were used. The
improvement by HOA adjustment was still evident.

% the Type I errors of the three large sample tests showed similar trends as
% seen in simulations where the true dispersion values were used.

Table \ref{tab:type-I-set} shows Monte Carlo type I error rates of tests for
two-group comparisons at nominal levels (alpha) $1\%$, $5\%$, $10\%$, and
$20\%$. Results are based on a simulated two-group data set containing NB
counts for $25,000$ genes. The two groups are of sizes $2$ and $4$.  The data
were simulated under the null hypothesis:
% both groups were simulated to have the same means.
for each gene $i$, read counts from all $6$ samples were simulated to have the
same mean, which was drawn from a log normal distribution ($\log_{10} \mu_i
\sim \N(2, 0.5)$), and the same dispersion, which was determined from the
equation $\phi_i = 1.5 \mu_i^{-0.5}$.  This dispersion model mimicked the
parametric dispersion model estimated from the Arabidopsis data (see
Section~\ref{sec:arab}).

When performing the test, we first estimate the dispersion model from all
$25,000$ genes combined using method described in \citet{Di:etal:2011}. The
estimated dispersion model is $\phi_i = 1.279  \mu_i^{-0.468}$. For each gene
$i$, we then plugged in $\hat\mu_i$ (simply the row means in this setting)
into this estimated mean-dispersion function to get the fitted value
$\hat\phi_i$ of $\phi_i$. Treating this estimated value $\hat\phi_i$ as known,
we performed HOA, LR and Wald tests to test three possible alternative
hypotheses: (a) the group with size $2$ has a smaller mean; (b) the group with
size $2$ has a greater mean; and (c) the two groups have different means.
Table~\ref{tab:type-I-set} summarizes the type I error rates of the tests over
all $25,000$ simulated genes.  The type I error rates show the same trends as
in the simulations where we treated $\phi$ as known. In particular, the HOA
test gives more accurate type I error rates.

% The reason for these surprisingly accurate Type I Errors (in spite of the
% estimation errors in $\phi$) might be that the effect of overestimation of
% $\phi_i$ for some genes is cancelled by the effect of underestimation of
% $\phi_i$ for some other genes with similar mean values.

% Note that in the this simulation, we still assumed that the form of the
%dispersion model is known and correct.  In Discussion and Conclusion, we will
%argue that it is reasonable to assume that dispersion ($\phi$) is a smooth
%function of the mean ($\mu$) or relative mean ($\mu$ divided by library
%size).  If a smooth function is correct, the linear function (on the log-log
%scale) should be approximately correct, at least for a moderate range of mean
%values.  We believe more research is needed to better understand the causes
%of overdispersion and to clarify the best approaches for dispersion modeling.


We use this example to outline the steps needed for practical application of
the NB regression model (see Section \ref{sec:model}).  We focus on one
biological question of interest: identify genes for which the effect of media
treatment differs in the two genotypes.  The statistical task is to test the
interaction term in a $2 \times 2$ design, which requires the use of the NB
regression model.  For the design matrix, we will let the intercept term
$X_{j1}=1$ for all samples $j=1, \dots, J$, $X_{j2}=1$ for one of the media
types,  $X_{j3}=1$ for one of the genotypes, and $X_{j4} = X_{j2}\times
X_{j3}$. We test the media-genotype interaction by testing the regression
coefficient corresponding to $X_{j4}$.  There is no exact NB test available
for testing the interaction term.

The regression model can account for differences in observed library sizes, so
there is no need to adjust read counts to make the library sizes equal. For
estimating the normalization factors $R_j$, we used the method described in
\citet{Anders:Huber:2010}, which assumes the median log fold change in
expression levels between any two samples is $0$. For this data set, this
method gives similar results  as the ``trimmed mean of M-values'' method of
\citet{Robinson:Oshlack:2010}.
%(relative differences are less than $2.5\%)$

For estimating the dispersion parameter, we assume a parametric dispersion
model
    where the $\pi_{ij} =
    \dfrac{\mu_{ij}}{N_j R_j}$ is the mean relative frequency after
    normalization. To estimate the parameters $\alpha = (\alpha_0, \alpha_1)$
    in the dispersion model, we maximize the {\em adjusted profile likelihood
    (APL)}: \begin{equation} \label{eq:apl} l_p(\alpha) = l(\alpha, \hat
	\beta_{\alpha}) - \dfrac{1}{2} \log \det\left\{j_{\beta \beta} (\hat
	\beta_{\alpha}; \alpha) \right\}, \end{equation} where $l(\alpha,
	\beta)$ is the full model likelihood, $\hat \beta_{\alpha}$ maximizes
	the constrained likelihood $l(\alpha, \beta)$ for fixed $\alpha$ and
	$j_{\beta \beta}$ the observed information matrix for estimating $\hat
	\beta_{\alpha}$. The expression of the likelihood function and other
	details are provided in Appendix~\ref{app:apl}. Adjusted profile
	likelihood was introduced by \citet{Cox:Reid:1987}, It can be viewed
	as an approximation to the conditional likelihood of $\alpha$ given
	the MLE of $\beta$.  In general, maximizing adjusted profile
	likelihood will give less biased estimates than maximizing unadjusted
	profile likelihood (i.e., $l(\alpha, \hat\beta_\alpha)$).
	\citet{Robinson:Smyth:2008} discussed its use in estimating the
	dispersion parameter in a constant dispersion model. For the bacteria
	data set, $(\alpha_0, \alpha_1)$ are estimated to be $(-1.857,
	0.037)$.

	
The estimated normalization factors $R_j$ and dispersion parameters
$\phi_{ij}$ are treated as known in the regression model~\ref{eq:mu}, and the
regression parameters are then estimated separately for each gene.
% can be fit to each gene and large-sample tests can be used to test for the
% regression coefficients.
We applied the LR and HOA tests to all genes and used false discovery rate
\citep[FDR,][]{Storey:Tibshirani:2003} to control for multiple testing.  With
a FDR cutoff of $0.05$, the HOA test identified $153$ genes where the effect
of the media has significant interaction with the genotype. For the $10$ genes
displaying the greatest interaction effect, Table~\ref{tab:log-fc} shows the
estimated log fold changes in their expression levels in the two media types
(KB rich versus minimal), separately for the two genotypes (wild type and hrpL
mutant).  The data provide strong evidence that for these genes the effect of
media treatment differs for the two genotypes. For this data set, the LR and
HOA tests give similar rankings of the genes, but the LR test $p$-values
tended to be smaller than corresponding HOA $p$-values.
% With the same FDR cutoff, the LR test identified all $153$ genes identified
% by the HOA test, together with $8$ additional ones ($5.2\%$ more).
LR test identified $161$ genes as showing interaction at the specified FDR of
$5\%$, including all $153$ identified by the HOA test. This is consistent with
the simulation results, which showed that type I error rates for the LR test
tended to be slightly larger than the nominal values (so the LR test would
produce more than $5\%$ false discoveries when FDR is set at $5\%$).


\section{Implementation details of the HOA test}
\label{app:hoa}
Here we give the expression of the adjustment term $z$ in Barndorff-Nielsen's
$r^*$ statistic with Skovgaard's approximations.
% to the sample space derivations involved.

Let $\beta = (\psi, \nu)$ where $\psi = (\beta_1, \dots, \beta_q)$ is the
parameter of interest and $\nu = (\beta_{q+1}, \dots, \beta_p)$ is a nuisance
parameter and we wish to test the null hypothesis $\psi = \psi_0$.  We let
$\hat\beta = (\hat\psi, \hat\nu)$ denote the maximum likelihood estimate of
the full parameter vector $\beta$ and  $\tilde\beta = (\psi_0, \tilde\nu)$
denote the maximum likelihood estimate of $\nu$ under the null hypothesis.
Let $l(\beta) = l(\beta; y)$ denote the log-likelihood, $D_1(\beta; y)$ denote
the score vector
\[ D_1(\beta; y)  = \dfrac{\partial}{\partial \beta} l(\beta; y),\]
and
$j(\beta)$ and $i(\beta)$ denote
denote the observed and the Fisher information matrices respectively:
\[ j(\beta) = j(\beta; y) = - \dfrac{\partial^2}{\partial \beta^2} l(\beta; y).\]
\[ i(\beta) = \var_\beta D_1(\beta; y) = - \E_\beta(j(\beta; y)). \]
With Skovgaard's approximations plugged in,
the general expression for the adjustment term $z$ in Barndorff-Nielsen's $r^*$
statistic $ r^* = r - \dfrac{1}{r} \log(z)$ is
\begin{equation}
    \label{eq:z}
    z \approx |j(\hat\beta)|^{-1/2}|i(\hat \beta)| |\hat S|^{-1} |
    j(\tilde\beta)_{\nu \nu}|^{1/2}
    \dfrac{r}{[\hat S^{-1} \hat q]_\psi},
\end{equation}
where $j(\tilde\beta)_{\nu\nu}$ refers to the submatrix corresponding to $\nu$
and the $[\hat S^{-1} \hat q]_{\psi}$ refers to the component corresponding to
$\psi$.
The two unfamiliar quantities in (\ref{eq:z}),
\[ \hat S = \cov_{\hat\beta}(D_1(\hat\beta; y), D_1(\tilde\beta; y)) \]
and
\[\hat q = \cov_{\hat\beta}(D_1(\hat\beta; y),l(\hat\beta; y)-l(\tilde\beta;
y)), \]
are approximations to the so-called sample space derivatives. Note that
the quantities involved in computing $z$ are similar to those involved in
computing the observed and Fisher information matrices.

% \[ \gamma = |\hat j|^{1/2} |\tilde D'|^{-1} |\tilde j_{vv}|^{1/2} \dfrac{r}{[(\hat l ' - \tilde l ')^T (\tilde D_1')^{-1}]_{\psi}}. \]
% $D_1'$ and $\hat l' - \tilde l'$
% \[ (\hat l' - \tilde l') \approx \hat q^T \hat i ^{-1} \hat j,
% \tilde D_1' \approx \hat S^T \hat i ^{-1} \hat j.\]

% which is exact when the model is a full exponential family.
Under the NB log-linear regression model~(\ref{eq:mu}) introduced in
Section~\ref{sec:model},
\[Y_j \sim \NB(\mu_j, \phi_j)\] \[\log(\mu_j)= \log (N_j R_j) + X_j^T \beta \] with
where $X_j = (X_{j1}, \dots, X_{jp})^T$, $\beta = (\beta_{1}, \dots,
\beta_p)^T$, and $\phi_j$'s are taken to be known,
% are vectors representing covariates and their effects on the mean expression level.
The likelihood from a single observation $y_j$, up to a constant that does
not depend on $\beta$, is
\begin{equation}
    \label{eq:lj}
    l_j(\beta; y_j) = y_j \log(\mu_j(\beta))  - (y_j+\kappa_j)\log(\mu_j(\beta)+\kappa_j)
\end{equation}
% \[ l_j(\beta; y_j)= y_j \log(p_j) - \kappa_j \log(1-p_j) \]
where $\kappa_j = 1/\phi_j$ and $\mu_j(\beta) = N_j R_j \exp(x_j^T \beta)$.
For a set of independent RNA-Seq counts $y=(y_1, \dots, y_n)$,
\begin{equation}
    \label{eq:l}
    l(\beta;y) = \sum_{j=1}^n l_j(\beta; y_j).
\end{equation}
The score vector is
\[ D_1(\beta; y) = \dpp{l}{\beta} = \sum_j \dpp{l_j}{\mu_j} \dpp{\mu_j}{\beta}
=\sum_j \dfrac{y_j - \mu_j}{\sigma_j^2} \mu_j x_j,\]
the observed information is
\begin{align}
    j(\beta) =  - \ddpp{l}{\beta}
    &= \sum_{j=1}^n \ddpp{l}{\mu_j} \dpp{\mu_j}{\beta} \dpp{\mu_j}{\beta}^T
    + \dpp{l}{\mu_j} \ddppp{\mu_j}{\beta}{\beta^T} \\
    &= \sum_{j=1}^n
    \left[
    \left(\dfrac{y_j}{\mu_j^2}  - \dfrac{y_j + \kappa_j}{(\mu_j + \kappa_j)^2} \right) \mu_j^2
    - \dfrac{(y_j - \mu_j) \mu_j}{\sigma_j^2}
    \right]
    x_j x_j^T,
    \label{eq:j}
\end{align}
and the Fisher information
\[ i(\beta) =  \sum_{j=1}^n  \dfrac{\mu_j^2}{\sigma_j^2} x_j x_j^T,\]
where $\sigma_j^2 = \mu_j + \phi_j \mu_j^2$.
The covariance of the score vectors evaluated at $\hat\beta$ and at
$\tilde\beta$ is
\[\hat S = \cov_{\hat\beta}(D_1(\hat\beta;Y), D_1(\tilde\beta;Y))
= \sum_j \hat\mu_j \tilde\mu_j \tilde\sigma_j^2x_j^T x_j
= \sum_j (1-\hat p_j )(1- \tilde p_j)\hat \sigma_j^2 x_j^T x_j. \]
and
\[\hat q =
\cov_{\hat\beta}(D_1(\hat\beta),l(\hat\beta)-l(\tilde\beta))=
\sum_j  \hat \mu_j  \log(\hat p_j/\tilde p_j) x_j. \]
where $p_j = p_j(\beta) = \log(\mu_j/(\mu_j + \kappa)$ and $\hat p_j =
p_j(\hat\beta)$ and $\tilde p_j = p_j(\tilde \beta)$.

% \[ l(\hat\beta; y) - l(\tilde\beta; y) = \sum_j y_j(\log(\hat p_j)- \log(\tilde p_j)). \]

For testing a hypothesis involving several parameters, \citet{Skovgaard:2001}
gave two generalizations:
\[ \lambda^{*} = \lambda - 2 \log \gamma\]
and
\[ \lambda^{**} = \lambda(1-\dfrac{1}{\lambda}\log\gamma)^2 \]
which are both approximately distributed as $\chi^2(q)$, where
$\gamma$ can be approximated by
\[ |\tilde i|^{1/2} |\hat i|^{1/2} |\hat S|^{-1} |\tilde j_{vv}|^{1/2}
|[\tilde i \hat S^{-1} \hat j \hat i \hat S ]_{vv}|^{-1/2}
\dfrac{ \{\tilde D_1^T \hat S^{-1} \hat i \hat j^{-1} \hat S \tilde i^{-1}
\tilde D_1 \}^{q/2}} {w^{q/2-1} \tilde D_1^T \hat S^{-1} \hat q}.\]
The second statistic $\lambda^{**}$ reduces to the square of $r^*$ when
$q=1$.

\section{NB regression models and full-rank exponential families}
\label{app:exp}
For comparing mean relative frequencies between two groups of NB counts,
$y_1, \dots, y_{n_1}$ and $y_{n_1 +1}, \dots, y_{n_1+n_2}$, if all
effective library sizes $N_j R_j$ are the same and all dispersion
parameters $\kappa_j$ are the same (say, equal to $\kappa$), the
log likelihood of the two
group means $\mu_1$ and $\mu_2$ is (cf. equations~(\ref{eq:lj}) and
(\ref{eq:l}))
\[ \sum_{j=1}^{n_1}  y_j  \log
\dfrac{\mu_1}{\mu_1+\kappa} - \sum_{j=n_1+1}^{n_2}  y_j  \log
\dfrac{\mu_2}{\mu_2+\kappa} - n_1 \kappa \log(\mu_1 + \kappa) - n_2
\kappa \log(\mu_2 + \kappa), \] which belongs to a two-dimensional
full-rank exponential family. (In this special case, a simpler formula
from \citet{Pierce:Peters:1992} can be used to compute the adjustment term
$z$, yielding identical results as using equation~(\ref{eq:z}) in
Appendix~\ref{app:hoa}.)

For more general NB regression models, each library will have a different
mean frequency $\mu_j$ and a different dispersion parameter $\kappa_j$,
so the joint distribution of $Y_j$'s does not belong to a full-rank
exponential family but rather to a curved exponential family.

%  After suitable reparameterization, the formula from Pierce and Peter
%  (1992) can be used to compute the HOA test statistic.

\section{Implementation Details of the Adjusted Profile Likelihood}
\label{app:apl}
In the adjusted profile likelihood~(\ref{eq:apl}), the expression for $j_{\beta
\beta}$ is given in~(\ref{eq:j}).
 In the dispersion model~(\ref{eq:dispersion}), preliminary
estimates of $\mu_{ij}$ are needed. One can estimate these quantities from
the regression model (\ref{eq:mu}) by assuming a constant dispersion
(e.g., $\phi=0.1$).
% One can refine the model with estimated dispersion parameters.
Precise mean estimates are unnecessary for the dispersion model since
under that model, the dispersion changes gradually with the mean.
%    The adjusted profile likelihood can be computed for other dispersion
%    models.
\bibliographystyle{bepress} \bibliography{nbp}


Now back to the NB data. If $Y_i \sim Poisson(Z_i)$, and $Z_i \sim
Gamma(\alpha_i, \beta_i)$,then
$Y_i$ will have a NB distribution. If $\beta$ is the same for all $i$, then
$\sum Z_i$ is
still a gamma random variable and $\sum Y_i \sim \Poisson (\sum Z_i)$ will have a NB2
distribution. More generally, if $\beta_i$ is not constant, the distribution
of $\sum Y_i$
may not belong to any known family of distributions. (If $\beta$ varies with
$\alpha$, but the function $\beta(\alpha)$ has a specific form,  $Y_i$ follows
a NBP distribution. But this fact is irrelevant here).

However, it is still true that $\sum Y_i \sim Poisson(\sum Z_i)$. If instead of model each
individual $Z_i$ as a gamma distribution, we simply model $\sum Z_i$ as a
gamma distribution, then $\sum Y_i$ will have a NB distribution. The second
assumption (assuming that $\sum Z_i$ has a gamma distribution) is not stronger than
the first assumption (assuming each individual $Z_i$ has a gamma
distribution). (Now what I look back, I think Anders and Huberâ€™s method can be
thought as making this assumption, they did not do library size adjustment.)

The point is that sometimes it might be possible to avoid a complex model by
making a different---but not stronger---assumption.

In regression setting, if the test statistic depends on certain linear
functions in $Y_I$,
$T = sum c_i Y_I$
then it is likely
$T ~ Poisson(sum (c_I Z_i))$
again can be approximated by a NB distribution.
\end{document}


