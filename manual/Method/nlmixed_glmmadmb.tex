%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=12.5pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm, bm} % Math packages
\usepackage{multirow}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{graphicx,epstopdf}
\usepackage{url}
\usepackage{color,soul}
\usepackage{sectsty} % Allows customizing section commands
\usepackage{hyperref}
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header


\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

% Setup the header and footer
\pagestyle{fancy}                                                       %
%\lhead{ST 565}                                                 %
\chead{Method}  %
%\rhead{Winter 2014}                                                     %

%\cfoot{}                                                                %
%\rfoot{Page\ \thepage\ of\ \pageref{LastPage}}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------
\title{Estimating random effect in Negative Binomial Regression}
%\author{Xihong Lin}
\date{} % Today's date or a custom date

\begin{document}
\maketitle
There are various ways of presenting the negative binomial distribution, and a detailed explanation can be found in Hilbe(2007). It can be viewed as a Poisson-Gamma mixture, which means we assume that $Y$'s are Poisson distributed with mean $\mu$ following a Gamma distribution. The density function is then expressed as
\[f(y; k,\mu)=\frac{\Gamma(y+ k)}{\Gamma(k)\Gamma(y + 1)}\times \left(\frac{k}{\mu + k }\right)^k\times\left(1- \frac{k}{\mu + k }\right)^y\]
where $\Gamma(y+1)=(y+1)!$.\\ The mean and variance of $Y$ are given by
\[E(Y)= \mu  ~~~~~~\text{Var}(Y)=\mu + \frac{\mu^2}{k}\]
Note that by letting $\phi= 1/k$ where $\phi$ is usually recognized as dispersion parameter,  we can reexpress the variance as $\text{Var}(Y)= \mu + \phi\mu^2$

\textbf{ Basic Setup of NB Regression}\\
The NB regression is specified with three steps
\begin{enumerate}
  \item $Y_i$ is negative binomial distributed with mean $\mu_i$ and dispersion parameter $\phi$.
  \item the predictor is given by $\eta = X\alpha + Zb$, where $\alpha$ and $b$ are fixed and random effects, respectively. Suppose $b\sim N(0, \sigma^2)$
  \item there is a link between the mean $Y$ and predictor $\eta = g(\mu)$, by default glmmadmb uses ....
\end{enumerate}

For  \href{http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default
/viewer.htm#statug_nlmixed_sect022.htm}{SAS} \verb"PROC NLMIXED" , the theory behind this procedure \\
\textbf{1. Assumptions and Notations}\\
  The observed data vector $\bm y_i$ for each $I$ subjects, $i=1, \ldots, s$ and $\bm y_i$ are assumed to be independent across $i$, but within-subject covariance is likely to exist because each of the elements of $\bm y_i$ is measured on the same subject( {\color{blue}{that implies it can deal with repeated measurement}}). The joint probability density function 
  \[p(\bm y_i|\bm X_i, \bm \phi, \bm u_i)q(\bm u_i|\bm \xi)\]
  where $\bm X_i$ is a matrix of observed explanatory variables and $\bm \phi, \bm \xi$ are vectors of unknown parameters.\\
  Let $\bm \theta =[\bm \phi, \bm \xi)]$ is of dimension $n$. Then we can make inference about $\bm\theta$ by the marginal likelihood funciton
  \[m(\bm \theta)= \prod_{i=1}^s\int p(\bm y_i|\bm X_i, \bm \phi, \bm u_i)q(\bm u_i|\bm \xi)d\bm u_i\]. Essentially we obtain $\hat{\bm \theta}$ by minimizing 
  $$f(\bm \theta)= -\log [m(\bm \theta)]$$
 Next we apply the NB regression setup here. For a particular gene $k$ (index suppressed here)
  \[y_{ij} \sim NB(\mu_i, \phi), ~~ \eta_i = \log(\mu_i)= \beta + e_i\]
  where $e_i\sim N(0, \sigma^2)$ and $i, j$ index $j$th sample in $i$th group. {\color{blue}{not sure if normalization is needed yet, in which case we might begin with $\log(\mu_i)= \beta + \log (N_i R_i) + e_i$}}. The data set I analyze consists of 4 labs of arabidopsis experiment with 2 or 3 samples in each lab. That being said, I am assuming the means for different labs vary only in terms of random effect $e_i$.  {\color{blue} Note: for now we just assume $\phi$ is a constant within different samples for a particular gene.}
  Note that $\bm \theta= (\beta, \sigma^2)$, and for SAS \verb"PROC NLMIXED" the NB $Y\sim \text{negbin}(n, p)$ log-likelihood is 
  \[  l(n, p; y)= \log[\Gamma(n+y)]- \log[\Gamma(n)] - \log[\Gamma(y +1)] + n\log(p) + y\log(1-p)\]
  \[E[Y] = kP = k\left(\frac{1-p}{p}\right), \text{Var}[Y] = kP(1-P) = k \left(\frac{1-p}{p}\right)\frac{1}{p}\] 
  with $n \geq 0 , 0 <p < 1$. That is equivalent to $\mu = kP, k = 1/\phi$  under NB2 parameterization.  Therefore the $p(\cdot)$ can be written as 
  \[p(y_i, p, k)= \frac{\Gamma(y+ k)}{\Gamma(k)\Gamma(y + 1)}\times p^k(1-p)^y\]
  By noting $\mu_i= e^{\beta + e_i}= k(1-p)/p$ we have  
  \[ p =  \frac{1}{k^{-1}e^{\beta  + e_i} + 1}\]
  Subsequently
  \[p(y_i| \beta, e_i, k)= \frac{\Gamma(y_i+ k)}{\Gamma(k)\Gamma(y_i + 1)}\times \left(\frac{1}{k^{-1}e^{\beta  + e_i} + 1}\right)^k\left(1-\frac{1}{k^{-1}e^{\beta  + e_i} + 1}\right)^{y_i}\]
  Now that $e_i\sim N(0, \sigma^2)$ with $q(e_i|\sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{e_i^2}{2\sigma^2}\right)$ gives 
    \[m(\beta, \sigma^2, k)=\prod_{i=1}^n\int p(y_i| \beta, e_i, k)q(e_i|\sigma^2)de_i\]  
    \[=\prod_{i=1}^n\int\frac{\Gamma(y_i+ k)}{\Gamma(k)\Gamma(y_i + 1)}\times \left(\frac{1}{k^{-1}e^{\beta  + e_i} + 1}\right)^k\left(1-\frac{1}{k^{-1}e^{\beta  + e_i} + 1}\right)^{y_i}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{e_i^2}{2\sigma^2}\right)de_i \]
  \textbf{2. Integration Approximation}\\
   \verb"PROC NLMIXED" uses \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.99&rep=rep1&type=pdf}{adaptive Gaussian Quadrature (Pinheiro and Bates 1995)} while R Package \verb"glmmADMB" adopts Laplace Approximation (reference ???). According to SAS documentation, the latter is just a $1^{\text{st}}$ order special case of the former. 
   Let $p(\beta, e_i) =\frac{1}{k^{-1}e^{\beta  + e_i} + 1}$, then rewriting $m(\beta, \sigma^2, k)$ gives 
   \[m(\beta, \sigma^2, k) = \prod_{i=1}^n\frac{\Gamma(y_i+ k)}{\Gamma(k)\Gamma(y_i + 1)}\int p(\beta, e_i)^k(1-p(\beta, e_i))^{y_i}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{e_i^2}{2\sigma^2}\right)~de_i\]  
   \[= \prod_{i=1}^n\frac{\Gamma(y_i+ k)}{\Gamma(k)\Gamma(y_i + 1)}\int e^{k\log[p(\beta,e_i)]}e^{y_i\log[1-p(\beta, e_i)]} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{e_i^2}{2\sigma^2}\right)~de_i \]
   \[=\prod_{i=1}^n\frac{\Gamma(y_i+ k)}{\Gamma(k)\Gamma(y_i + 1)}\frac{1}{\sqrt{2\pi\sigma^2}}\int \exp\left[k\log (p) + y_i\log(1-p) - \frac{e_i^2}{2\sigma^2}\right]~de_i\eqno(\ast)\]
   The integral is approximated by Gaussian Quadrature. Denote
    $$l(e_i, y_i)=k\log(p) + y_i\log(1-p)-\frac{e_i^2}{2\sigma^2}$$
   Let $e_i^{\ast}$ maximizes $l(e_i, y_i)$., then  $(\ast)$ can be approximated by 
   \[ \text{Gaussian Quadrature here}\]
   Can try
   \begin{verbatim}
   file.show(system.file("tpl","glmmadmb.tpl",package="glmmADMB"))" 
   \end{verbatim}
   to see how parameters are estimated in \verb"glmmADMB".\\
   It seems the \href{http://www.admb-project.org/documentation/manuals}{ADMB-RE} package (implementing random effects in nonlinear models) is also adaptive to non-normally distributed random effects and C++ programs are available. 
   SAS code for NB regression 
  %\begin{verbatim}
  % proc nlmixed;
  %    parms b0=1 b1=0 scale=14; /*initiating parameters*/
  %    linp = b0 + b1*x; /*link function*/
  %    mu   = exp(linp);
  %    b    = mu/scale;
  %    model y ~ gamma(scale,b);
  % run;
  %\end{verbatim}
\end{document}
